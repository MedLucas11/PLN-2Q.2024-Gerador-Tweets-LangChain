{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PcCTmXAl8Acl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Langchain in Chains: Google Search**\n",
        "###Gerador de tweets automatizado com LLMs\n",
        "---"
      ],
      "metadata": {
        "id": "IDcMDwFnfeD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objetivo**\n",
        "---\n",
        "Vamos criar um gerador autom√°tico de tweets, utilizando como base um assunto qualquer de nossa escolha, para isso vamos utilizar o ***Google Serper API***, uma API que permite recuperar resultados de uma pesquisa no Google:\n",
        "\n",
        "<center><img src='https://cdn.prod.website-files.com/659415b46df8ea43c3877776/65b7f02e3e78f985c9b5b108_serper-homepage.jpeg'></center>\n",
        "\n",
        "Al√©m disso vamos utilizar o LangChain para facilitar a implementa√ß√£o, juntamente com os LLMs Cohere para realizar embeddings e Gemini para gera√ß√£o dos tweets\n",
        "\n"
      ],
      "metadata": {
        "id": "-WwAJU52gy9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Metodologia**\n",
        "---\n",
        "Seguimos os m√©todos presentes no artigo escolhido<sup>[1]</sup>, com algumas diferen√ßas para poder utilizar LLMs gratuitos, j√° que o artigo utiliza OpenAI. Por conta dessa mudan√ßa de LLM algumas partes do c√≥digo ficaram diferentes do artigo original, mas o resultado final √© o mesmo, os passos seguidos s√£o:\n",
        "\n",
        "#### 1. Fazer o download das bibliotecas necess√°rias\n",
        "\n",
        "#### 2. Setar as chaves de API que iremos utilizar\n",
        "  \n",
        "#### 3. Definir uma fun√ß√£o que ir√° realizar a busca no Google, de acordo com um assunto passado como entrada\n",
        "\n",
        "#### 4. Definir uma fun√ß√£o que a partir dos resultados da busca retorna uma lista de 3 URLs mais relevantes\n",
        "\n",
        "#### 5. Definir a fun√ß√£o que ir√° separar os textos das URLs em blocos e realizar os embeddings, retornando um objeto ***vectorstore*** com FAISS\n",
        "\n",
        "#### 6. Definir a fun√ß√£o que ir√° gerar os tweets, fazendo uma busca por similaridade entre a query que fizemos no Google e o ***vectorstore*** criado no passo 5, juntando tudo em um √∫nico documento e criando os tweets utilizando a intelig√™ncia artificial\n",
        "\n"
      ],
      "metadata": {
        "id": "kTKwSL3NjSmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FAISS (Facebook AI Similarity Search)**\n",
        "---\n",
        "<center><img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*i7BwsiaZ71mCuq12enDJtA.jpeg'></center>\n",
        "\n",
        "Como dito acima, vamos criar um ***vectorstore*** com FAISS para poder realizar uma busca por similaridade depois, vamos entender melhor o que √© isso.\n",
        "\n",
        "FAISS √© uma biblioteca desenvolvida pela Meta AI, projetada especifamente para lidar com pesquisas de similaridade de forma eficiente, o que √© √∫til para lidar com grandes conjuntos de dados.\n",
        "\n",
        "Ele pode ser utilizado para criar um √≠ndice e realizar pesquisas com grande velocidade e efici√™ncia de mem√≥ria.\n",
        "\n",
        "O FAISS agiliza as pesquisas de vizinhos mais pr√≥ximos indexando vetores usando algoritmos sofisticados como:\n",
        "\n",
        "1. **K-means clustering**: Esse algoritmo divide os dados em clusters, o que ajuda a restringir o espa√ßo de pesquisa, concentrando-se nos clusters mais relevantes.\n",
        "\n",
        "2. **Quantiza√ß√£o de produto (PQ)**: O PQ comprime os vetores em c√≥digos mais curtos, reduzindo significativamente o uso da mem√≥ria e acelerando a pesquisa sem uma grande queda de precis√£o\n",
        "\n",
        "3. **Quantiza√ß√£o otimizada de produtos (OPQ)**: Uma vers√£o aprimorada do PQ, o OPQ gira os dados para se ajustar melhor √† grade de quantiza√ß√£o, melhorando a precisa√ß√£o de vetores compactados.\n",
        "\n",
        "\n",
        "### **Casos de uso do FAISS**\n",
        "Por ser muito vers√°til e eficiente, o FAISS √© ideal para uma variedade de aplica√ß√µes de diferente tipos como:\n",
        "\n",
        "- Sistemas de Recomenda√ß√£o\n",
        "- Pesquisa de imagens e v√≠deos\n",
        "- Detec√ß√£o de anomalias\n",
        "- Recupera√ß√£o de informa√ß√µes (Utilizado neste projeto!)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hu9paRTDuyYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementa√ß√£o**\n",
        "---"
      ],
      "metadata": {
        "id": "UTuCJCQX52G3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1¬∫ passo:\n",
        "Instalar as bibliotecas necess√°rias e importar o que vai ser utilizado:"
      ],
      "metadata": {
        "id": "b3dpmlQS66ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-cohere\n",
        "!pip install -qU langchain-google-genai\n",
        "!pip install -qU unstructured\n",
        "!pip install -qU faiss-cpu\n",
        "!pip install -qU faiss-gpu\n",
        "!pip install -qU langchain\n",
        "!pip install -qU beautifulsoup4"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_oJOqrideDhK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.utilities import GoogleSerperAPIWrapper\n",
        "from langchain import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from bs4 import BeautifulSoup\n",
        "from ast import literal_eval\n",
        "from google.colab import userdata\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "tIHRZZDYfV2q"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2¬∫ passo:\n",
        "Setar as chaves para utilizar os servi√ßos das APIs, tamb√©m definir o modelo que ir√° realizar os embeddings:"
      ],
      "metadata": {
        "id": "RxxQ2iOy7cqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = userdata.get(\"COHERE_API_KEY\")\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get(\"SERPER_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "yJLZoWiVfr82"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")"
      ],
      "metadata": {
        "id": "wrTrzsCMgW1d"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3¬∫ passo:\n",
        "\n",
        "Definindo a fun√ß√£o que realiza a busca baseado em um assunto de entrada:"
      ],
      "metadata": {
        "id": "PcCTmXAl8Acl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A fun√ß√£o possui um par√¢metro \"query\" que ser√° a nossa busca desejada\n",
        "def search(query: str) -> dict:\n",
        "\n",
        "  # Instaciamos o objeto GoogleSerperAPIWrapper, que ir√° realizar a busca baseada na query\n",
        "  # k ser√° a quantidade de resultados, gl o pa√≠s de origem das p√°ginas e hl a l√≠ngua\n",
        "  search = GoogleSerperAPIWrapper(k=10, gl='br', hl='pt-br', type=\"search\")\n",
        "  response = search.results(query)\n",
        "\n",
        "  # Por fim retornamos os resultados\n",
        "  return response"
      ],
      "metadata": {
        "id": "d7w4pc82gcUW"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui definimos nossa busca e utilizamos a fun√ß√£o criada para\n",
        "query = \"√öltimas not√≠cias do futebol brasileiro\"\n",
        "search_results = search(query)\n",
        "search_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOQQrkc6grx8",
        "outputId": "f393ca4e-c7dc-4c15-a120-3d77eae9c42c"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'searchParameters': {'q': '√öltimas not√≠cias do futebol brasileiro',\n",
              "  'gl': 'br',\n",
              "  'hl': 'pt-br',\n",
              "  'type': 'search',\n",
              "  'num': 10,\n",
              "  'engine': 'google'},\n",
              " 'organic': [{'title': 'Futebol | ge - Globo Esporte',\n",
              "   'link': 'https://ge.globo.com/futebol/',\n",
              "   'snippet': 'Acompanhe as not√≠cias de Futebol no ge.globo.',\n",
              "   'sitelinks': [{'title': 'Futebol internacional',\n",
              "     'link': 'https://ge.globo.com/futebol/futebol-internacional/'},\n",
              "    {'title': 'Brasileiro Feminino',\n",
              "     'link': 'https://ge.globo.com/futebol/futebol-feminino/brasileiro-feminino/'},\n",
              "    {'title': 'Futebol portugu√™s',\n",
              "     'link': 'https://ge.globo.com/futebol/futebol-internacional/futebol-portugues/'},\n",
              "    {'title': 'Flamengo',\n",
              "     'link': 'https://ge.globo.com/futebol/times/flamengo/'}],\n",
              "   'position': 1},\n",
              "  {'title': 'Futebol - UOL Esporte',\n",
              "   'link': 'https://www.uol.com.br/esporte/futebol/ultimas/',\n",
              "   'snippet': 'Acompanhe no UOL Esporte as √öltimas not√≠cias, fotos, v√≠deos, dicas e competi√ß√µes de futebol e provas de outras modalidades.',\n",
              "   'position': 2},\n",
              "  {'title': 'Futebol Nacional | Not√≠cias, jogos e resultados - LANCE!',\n",
              "   'link': 'https://www.lance.com.br/futebol-nacional',\n",
              "   'snippet': 'No LANCE, voc√™ fica por dentro de tudo sobre Futebol Nacional. Acompanhe as √∫ltimas not√≠cias, resultados e pr√≥ximos jogos. Clique e n√£o perca nada!',\n",
              "   'position': 3},\n",
              "  {'title': 'Tudo sobre: Futebol brasileiro - CNN Brasil',\n",
              "   'link': 'https://www.cnnbrasil.com.br/tudo-sobre/futebol-brasileiro/',\n",
              "   'snippet': 'Tudo sobre Futebol brasileiro. Conte√∫dos, not√≠cias, entrevistas, breaking news e mais. Acompanhe na CNN Brasil!',\n",
              "   'position': 4},\n",
              "  {'title': 'Campeonato Brasileiro - Not√≠cias, Estat√≠sticas e Resultados - ESPN',\n",
              "   'link': 'https://www.espn.com.br/futebol/liga/_/nome/bra.1',\n",
              "   'snippet': 'Siga tudo sobre Campeonato Brasileiro. Not√≠cias, classifica√ß√£o, estat√≠sticas e mais na ESPN.',\n",
              "   'position': 5},\n",
              "  {'title': 'ge.globo - √â esporte sempre',\n",
              "   'link': 'https://ge.globo.com/',\n",
              "   'snippet': 'No ge.globo voc√™ encontra a melhor cobertura sobre o Futebol e Outros Esportes, no Brasil e no Mundo: Not√≠cias, V√≠deos, Tabelas, Agenda e muito mais.',\n",
              "   'sitelinks': [{'title': 'Brasileir√£o S√©rie A',\n",
              "     'link': 'https://ge.globo.com/futebol/brasileirao-serie-a/'},\n",
              "    {'title': 'Brasileiro Feminino',\n",
              "     'link': 'https://ge.globo.com/futebol/futebol-feminino/brasileiro-feminino/'},\n",
              "    {'title': 'Futebol italiano',\n",
              "     'link': 'https://ge.globo.com/futebol/futebol-internacional/futebol-italiano/'},\n",
              "    {'title': 'Flamengo',\n",
              "     'link': 'https://ge.globo.com/futebol/times/flamengo/'}],\n",
              "   'position': 6},\n",
              "  {'title': '90min - As √∫ltimas not√≠cias de futebol, contrata√ß√µes e opini√£o do ...',\n",
              "   'link': 'https://www.90min.com/pt-BR',\n",
              "   'snippet': '90min √© a maior plataforma de futebol do mundo, reunindo as √∫ltimas not√≠cias, rumores de transfer√™ncias, jogos, resultados, opini√£o, v√≠deos e muito ...',\n",
              "   'sitelinks': [{'title': 'Not√≠cias de Futebol',\n",
              "     'link': 'https://www.90min.com/pt-BR/categories/noticias-futebol'},\n",
              "    {'title': 'Futebol internacional',\n",
              "     'link': 'https://www.90min.com/pt-BR/categories/futebol-internacional'},\n",
              "    {'title': 'Brasileiro S√©rie B',\n",
              "     'link': 'https://www.90min.com/pt-BR/leagues/brasileiro-serie-b'},\n",
              "    {'title': 'Brasileir√£o S√©rie A',\n",
              "     'link': 'https://www.90min.com/pt-BR/leagues/brasileiro-serie-a'}],\n",
              "   'position': 7},\n",
              "  {'title': 'Mercado da Bola - Transfer√™ncias da s√©rie A do Brasileir√£o - LANCE!',\n",
              "   'link': 'https://www.lance.com.br/mercado-da-bola/',\n",
              "   'snippet': 'Acompanhe o vai e vem entre os clubes de futebol em 2024, janela de transfer√™ncias e todas as informa√ß√µes relevantes sobre os jogadores. Destaque. √öltimas ...',\n",
              "   'sitelinks': [{'title': 'Flamengo busca contrata√ß√£o...',\n",
              "     'link': 'https://www.lance.com.br/flamengo/flamengo-busca-contratacao-de-lucas-paqueta-na-janela-de-transferencias-entenda.html'},\n",
              "    {'title': 'Ap√≥s a sa√≠da de Jo√£o Palhinha',\n",
              "     'link': 'https://www.lance.com.br/futebol-internacional/bayern-acerta-com-joao-palhinha-e-fulham-volta-a-manifestar-interesse-em-volante-brasileiro.html'},\n",
              "    {'title': 'Corinthians',\n",
              "     'link': 'https://www.lance.com.br/mercado-da-bola/estatisticas/corinthians'}],\n",
              "   'position': 8},\n",
              "  {'title': 'FutebolBrasileiro | Not√≠cias - Somos Fan√°ticos',\n",
              "   'link': 'https://somosfanaticos.fans/br/futebolbrasileiro',\n",
              "   'snippet': '√öltimas noticias de FutebolBrasileiro. Conhe√ßa todas as novidades relacionadas a FutebolBrasileiro sobre somosfanaticos.fans/br.',\n",
              "   'position': 9},\n",
              "  {'title': 'Campeonato Brasileiro S√©rie A: Not√≠cias e Jogos do Brasileir√£o | Terra',\n",
              "   'link': 'https://www.terra.com.br/esportes/futebol/brasileiro-serie-a/',\n",
              "   'snippet': 'Confira todas as informa√ß√µes sobre o Brasileir√£o S√©rie A: tabela do campeonato, classifica√ß√£o e resultados dos jogos, not√≠cias, fotos, v√≠deos e mais.',\n",
              "   'sitelinks': [{'title': 'Classifica√ß√£o e Tabela',\n",
              "     'link': 'https://www.terra.com.br/esportes/futebol/brasileiro-serie-a/tabela/'},\n",
              "    {'title': 'S√©rie B',\n",
              "     'link': 'https://www.terra.com.br/esportes/futebol/brasileiro-serie-b/'},\n",
              "    {'title': 'Neto crava favorito para...',\n",
              "     'link': 'https://www.terra.com.br/esportes/futebol/brasileiro-serie-a/neto-crava-favorito-para-vencer-o-brasileirao-clube-nao-leva-o-titulo-ha-mais-de-20-anos,0c6d3e931b0214384055f3ded708aac5e9iods96.html'}],\n",
              "   'position': 10}],\n",
              " 'peopleAlsoAsk': [{'question': 'Quais s√£o as √∫ltimas not√≠cias sobre futebol?',\n",
              "   'snippet': 'Cristiano Ronaldo ironiza o pr√≥prio time e n√£o participa de premia√ß√£o na Supercopa Saudita. ...\\nItuano quebra tr√™s tabus em dois jogos e ganha f√¥lego contra o rebaixamento na S√©rie B. ...\\nVc Escala: com Almada e trio de volantes, torcida do Botafogo quer for√ßa m√°xima contra o Flamengo.',\n",
              "   'title': 'Futebol | ge - Globo Esporte',\n",
              "   'link': 'https://ge.globo.com/futebol/'},\n",
              "  {'question': 'Quais s√£o as not√≠cias do esporte de hoje?',\n",
              "   'snippet': '√öLTIMAS NOT√çCIAS DE: ESPORTES\\nTitular do Atl√©tico-MG, meia Mat√≠as Zaracho passar√° por cirurgia. ...\\nBrighton x Manchester United: hor√°rio e onde assistir ao jogo da Premier League. ...\\nTati Weston-Webb √© vice em Fiji e garante vaga no WSL Finals. ...\\nSantos x Amazonas: hor√°rio e onde assistir ao jogo da S√©rie B. ...\\nD√©j√† vu?',\n",
              "   'title': 'Esportes - Confira not√≠cias da Editoria - CNN Brasil',\n",
              "   'link': 'https://www.cnnbrasil.com.br/esportes/ultimas-noticias/'},\n",
              "  {'question': 'Qual o melhor site de not√≠cias de futebol?',\n",
              "   'snippet': 'Bem-vindo ao OneFootball. Sua melhor fonte de not√≠cias de futebol, jogos ao vivo, resultados, destaques e muito mais!',\n",
              "   'title': 'OneFootball: inicio',\n",
              "   'link': 'https://onefootball.com/pt-br/inicio'},\n",
              "  {'question': 'Quais s√£o os jogos de hoje pelo Brasil?',\n",
              "   'snippet': 'ESPORTE NA BAND\\nAtl√©tico-MG x Fluminense: placar ao vivo, escala√ß√µes, lances, gols e mais.\\nPalmeiras x Cuiab√°: placar ao vivo, escala√ß√µes, lances, gols e mais.\\nItuano x Goi√°s: placar ao vivo, escala√ß√µes, lances, gols e mais.\\nSantos x Amazonas: placar ao vivo, escala√ß√µes, lances, gols e mais.',\n",
              "   'title': 'Jogos de hoje - Veja hor√°rios e partidas - Band - UOL',\n",
              "   'link': 'https://www.band.uol.com.br/esportes/futebol/jogos-de-hoje'}],\n",
              " 'relatedSearches': [{'query': '√öltimas not√≠cias sobre futebol'},\n",
              "  {'query': 'Not√≠cias do futebol brasileiro mercado da bola'},\n",
              "  {'query': 'Not√≠cias futebol europeu'},\n",
              "  {'query': 'Not√≠cias do futebol internacional'},\n",
              "  {'query': '√öltimas not√≠cias sobre futebol Flamengo'},\n",
              "  {'query': 'Tudo sobre futebol brasileiro'},\n",
              "  {'query': 'Google not√≠cias futebol'},\n",
              "  {'query': 'Jogos de hoje ao vivo'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4¬∫ passo:\n",
        "\n",
        "Definir uma fun√ß√£o que, a partir dos resultados da pesquisa, retorna apenas 3 URLs que s√£o consideradas mais relevantes:"
      ],
      "metadata": {
        "id": "hPtYxR9OACF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passamos como par√¢metro os resultados da pesquisa assim como a \"query\" que foi feita\n",
        "def extract_top_article_urls(search_results: dict, query: str) -> list:\n",
        "\n",
        "  # Primeiros tranformamos os resultados em uma string, para poder passar no template para Intelig√™ncia Artificial\n",
        "  string = json.dumps(search_results)\n",
        "\n",
        "  # Definimos o modelo que ser√° utilizado para nossa tarefa\n",
        "  modelo = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.8)\n",
        "\n",
        "  # Escrevemos um template de prompt, ele ir√° analisar os resultados obtidos na nossa pesquisa e filtrar pelos 3 que ele considerar mais relevantes baseado na nossa pesquisa inicial, retornando uma lista de URLs\n",
        "  prompt_template_str = \"\"\"\n",
        "  You are a world-class sports journalist with a keen eye for identifying content that drives views and interactions on Twitter. Your expertise lies in finding the most engaging, relevant, and timely sports articles.\n",
        "\n",
        "  QUERY RESPONSE: {string}\n",
        "\n",
        "  Above is the list of search results for the query \"{query}\".\n",
        "\n",
        "  Your task is to meticulously select the top 3 articles from the list that will generate the highest engagement on Twitter. Return ONLY a LIST (ARRAY) of the URLs. Exclude any other information. Ensure the articles are fresh and not outdated. If the file or URL is invalid, return www.google.com.\n",
        "  \"\"\"\n",
        "\n",
        "  # Criamos a cadeia que ir√° executar o template com os par√¢metros necess√°rios, juntamente com o modelo que ser√° utilizado\n",
        "  prompt_template = PromptTemplate(input_variables=[\"string\", \"query\"], template=prompt_template_str)\n",
        "  selection_chain =  prompt_template | modelo\n",
        "\n",
        "  # Executamos a cadeia\n",
        "  response = selection_chain.invoke({\"string\": string, \"query\": query})\n",
        "\n",
        "  # Como a resposta que √© retornada vem em uma String deste tipo \" ```python[\"url1\", \"url2\", \"url3\"]``` \", fazemos um tratamento para ficar no formato de lista de fato\n",
        "  urls = response.content\n",
        "  urls = urls.replace(\"`\", \"\")\n",
        "  urls = urls.replace(\"json\", \"\")\n",
        "\n",
        "  # Ap√≥s o tratamento ele ainda √© uma String, por isso utilizamos essa fun√ß√£o para transformar uma String diretamente em Lista\n",
        "  urls_list = literal_eval(urls)\n",
        "\n",
        "  return urls_list"
      ],
      "metadata": {
        "id": "cZNnKG4ohcC7"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls_list = extract_top_article_urls(search_results, query)\n",
        "print(urls_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U36MF8GwinK-",
        "outputId": "9eff7117-5f54-4ed8-addc-bf595c02be07"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://ge.globo.com/futebol/', 'https://www.uol.com.br/esporte/futebol/ultimas/', 'https://www.lance.com.br/futebol-nacional']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5¬∫ passo:\n",
        "Definir fun√ß√µes para extrair os textos das URLs e para separar os textos, criando o ***vectorstore***:"
      ],
      "metadata": {
        "id": "WwF8aGNrMAsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Essa primeira fun√ß√£o serve para recuperar o conte√∫do das URLs\n",
        "def fetch_text_from_url(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_0) AppleWebKit/536.1 (KHTML, like Gecko) Chrome/58.0.849.0 Safari/536.1'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "\n",
        "# Toda essa pr√≥xima parte foi feita com aux√≠lio do ChatGPT. Utilizando os embeddings do Cohere diversos erros aconteciam ao escrever o c√≥digo igual ao do artigo, mais especificamente ao tentar instaciar o objeto FAISS.\n",
        "# Por isso fomos implementando as sugest√µes feitas pela IA para tentar contornar os erros e obter o resultado esperado.\n",
        "\n",
        "# Criamos uma classe que simula um armazenamento de documentos que √© suportado pelo √≠ndice FAISS, oferecendo duas fun√ß√µes principais 'get' e 'search'\n",
        "class DummyDocStore:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "\n",
        "    # Recupera um documento espec√≠fico com base em seu ID, verificando se o ID est√° dentro dos limites da lista de documentos\n",
        "    def get(self, doc_id):\n",
        "        if 0 <= doc_id < len(self.documents):\n",
        "            return self.documents[doc_id]\n",
        "        else:\n",
        "            raise ValueError(f\"Document ID {doc_id} is out of bounds.\")\n",
        "\n",
        "    # Recupera documentos com base em uma lista de IDs.\n",
        "    def search(self, doc_ids):\n",
        "\n",
        "        # Verifica se 'doc_ids' √© um inteiro (o que indica apenas um documento)\n",
        "        if isinstance(doc_ids, int):\n",
        "            return self.get(doc_ids)  # Retorna um √∫nico documento\n",
        "\n",
        "        # Se 'doc_ids' for uma lista, retorna uma lista de documentos correspondente\n",
        "        elif isinstance(doc_ids, list):\n",
        "            return [self.get(doc_id) for doc_id in doc_ids]\n",
        "        else:\n",
        "            raise ValueError(\"doc_ids should be an int or list of ints.\")\n",
        "\n",
        "\n",
        "# Fun√ß√£o para carregar e processar os conte√∫dos das URLs, dividir o texto em chunks, criar os embeddings e construir um √≠ndice FAISS para busca\n",
        "def split_content_from_urls(urls_list: list) -> FAISS:\n",
        "\n",
        "  # Adicionamos os textos brutos das URLs em uma lista\n",
        "  texts = [fetch_text_from_url(url) for url in urls_list]\n",
        "\n",
        "  # Criamos objetos 'Document' a partir dos textos\n",
        "  documents = [Document(page_content=text) for text in texts]\n",
        "\n",
        "  # Definimos o splitter que ir√° dividir os textos. Esse m√©todo faz o split de forma recursiva, e tenta manter 'peda√ßos' relacionados pr√≥ximos um do outro\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "  # Iniciamos listas vazias que ir√£o armazenar os chunks de texto e os IDs dos documentos originais\n",
        "  split_documents = []\n",
        "  doc_ids = []\n",
        "\n",
        "  for i, doc in enumerate(documents):\n",
        "      chunks = text_splitter.split_text(doc.page_content) # Divide o conte√∫do de cada documento em chunks\n",
        "      split_documents.extend(chunks) # Adiciona os chunks gerados √† lista 'split_documents'\n",
        "      doc_ids.extend([i] * len(chunks)) # Associa o ID do documento original para cada chunk gerado a partir dele. Isso permite mapear cada chunk de volta ao documento original\n",
        "\n",
        "  # Cria embeddings para cada chunk de texto usando o modelo de embeddigns do Cohere\n",
        "  chunk_embeddings = embeddings.embed_documents(split_documents)\n",
        "\n",
        "  # Converte os embeddings para um array numpy de ponto flutuante, que √© necess√°rio para criar o √≠ndice FAISS\n",
        "  chunk_embeddings = np.array(chunk_embeddings, dtype=np.float32)\n",
        "\n",
        "  # Colocamos a dimens√£o dos nossos embeddings em uma vari√°vel (n√∫mero de colunas)\n",
        "  dim = chunk_embeddings.shape[1]\n",
        "\n",
        "  # Criamos um √≠ndice FAISS utilizando a m√©trica de dist√¢ncia L2 (dist√¢ncia euclidiana) para compara√ß√£o\n",
        "  faiss_index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "  # Adicionamos os embeddings ao nosso √≠ndice FAISS\n",
        "  faiss_index.add(chunk_embeddings)\n",
        "\n",
        "  # Utilizamos a classe que criamos para armazenar os documentos originais\n",
        "  docstore = DummyDocStore(documents)\n",
        "\n",
        "  # Criamos um dicion√°rio que mapeia os IDs dos chunks de texto para os IDs dos documentos originais\n",
        "  index_to_docstore_id = {i: doc_ids[i] for i in range(len(split_documents))}\n",
        "\n",
        "  # Instanciamos o objeto FAISS, com todos os par√¢metros necess√°rios\n",
        "  faiss_store = FAISS(index=faiss_index, docstore=docstore, index_to_docstore_id=index_to_docstore_id, embedding_function=embeddings)\n",
        "\n",
        "  return faiss_store"
      ],
      "metadata": {
        "id": "VKI-Lpw0p_Kd"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = split_content_from_urls(urls_list)\n",
        "print(type(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RISnlyJV66GP",
        "outputId": "d08df55a-f237-4af9-d342-84514703c4f8"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_community.vectorstores.faiss.FAISS'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6¬∫ passo:\n",
        "Definir a fun√ß√£o que ir√° gerar os tweets:"
      ],
      "metadata": {
        "id": "io4kChHrOvep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finalmente, definimos a fun√ß√£o que ir√° gerar os tweets, baseado no √≠ndice FAISS para buscar documentos semelhantes, a 'query' que fizemos e o n√∫mero 'k' de documentos semelhantes que desejamos recuperar\n",
        "def generate_engaging_tweets(faiss_index: FAISS, query: str, k: int=3) -> str:\n",
        "\n",
        "    # Busca os documentos mais semelhantes com a 'query' que foi feita\n",
        "    similar_docs = faiss_index.similarity_search(query, k=k)\n",
        "\n",
        "    # Verifica se 'similar_docs' √© uma lista ou apenas um documento\n",
        "    if isinstance(similar_docs, list):\n",
        "        aggregated_content = \" \".join([doc.page_content for doc in similar_docs if doc is not None])\n",
        "    else:\n",
        "        aggregated_content = similar_docs.page_content if similar_docs is not None else \"\"\n",
        "\n",
        "    # Caso ocorra erro pelo tamanho do conte√∫do, determine um limite para n√£o ultrapassar a cota do LLM\n",
        "    #if len(aggregated_content) > 10000:\n",
        "      #aggregated_content = aggregated_content[:10000]\n",
        "\n",
        "\n",
        "    # Definimos o modelo que ser√° utilizado, o prompt, e criamos a chain que ir√° gerar os tweets\n",
        "    modelo = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.8)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "        {docs}\n",
        "        As a world-class sports journalist, you will summarize the text above to create engaging tweets around the topic \"{query}\".\n",
        "        These tweets will be posted on Twitter to drive high engagement.\n",
        "        Write them in brazilian portuguese.\n",
        "\n",
        "        Please follow all of the following guidelines:\n",
        "        1. Ensure the content is engaging and informative with good data.\n",
        "        2. Keep the tweets concise, fitting within Twitter's character limit.\n",
        "        3. Address the topic \"{query}\" very well and stay on point.\n",
        "        4. Make sure the content is high quality and informative.\n",
        "        5. Write in a way that is easy to read, digest, and understand.\n",
        "        6. Provide actionable insights and advice, including resources and links if necessary.\n",
        "\n",
        "        TWEET:\n",
        "        \"\"\"\n",
        "\n",
        "    prompt_template = PromptTemplate(input_variables=[\"docs\", \"query\"], template=prompt_template_str)\n",
        "\n",
        "    tweet_summarizer_chain = prompt_template | modelo\n",
        "\n",
        "    tweets = tweet_summarizer_chain.invoke({\"docs\":aggregated_content, \"query\":query})\n",
        "\n",
        "    return tweets.content"
      ],
      "metadata": {
        "id": "r7_jhNTA_js9"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = generate_engaging_tweets(data, query)\n",
        "Markdown(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "mqKwa3sY__Xj",
        "outputId": "0b3c8321-ea1d-4a46-873a-d929f1c1601e"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## √öltimas not√≠cias do futebol brasileiro:\n\n**Tweet 1:** üö® O Inter n√£o perde para o Cruzeiro no Beira-Rio h√° mais de 10 anos! üòÆ Ser√° que o tabu se mant√©m? #Inter #Cruzeiro #Grenal #Brasileir√£o\n\n**Tweet 2:** üè• Tite teve alta do hospital ap√≥s interna√ß√£o por arritmia card√≠aca. üôè Desejamos melhoras ao t√©cnico! #Tite #Sele√ß√£oBrasileira #For√ßaTite\n\n**Tweet 3:** üî• Flamengo faz proposta por Alex Sandro, ex-Juventus! üí∞ Ser√° que o Meng√£o vai repatriar o lateral? #Flamengo #AlexSandro #MercadoDaBola\n\n**Tweet 4:** ‚öΩÔ∏è  O Corinthians vendeu Wesley para a Ar√°bia Saudita! üá∏üá¶ O jogador ser√° companheiro de Cristiano Ronaldo! #Corinthians #Wesley #CristianoRonaldo\n\n**Tweet 5:** üèÜ Libertadores: cariocas complicam datas das quartas de final. ü§î CBF busca alternativas! #Libertadores #Flamengo #Fluminense #CBF \n\n**Tweet 6:** üèüÔ∏è  Jogo do Fluminense hoje: saiba onde assistir, hor√°rio e escala√ß√µes! ‚û°Ô∏è [link para o artigo] #Fluminense #Brasileir√£o #OndeAssistir\n\n**Tweet 7:** üìä Convoca√ß√£o para Sele√ß√£o comprova acerto de Luiz Henrique ao escolher o Botafogo! #Botafogo #LuizHenrique #Sele√ß√£oBrasileira\n\n**Tweet 8:** üö® Santos ter√° novidade em todas as √°reas do campo contra o Amazonas! #Santos #S√©rieB #FutebolBrasileiro\n"
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ref√™rencias:**\n",
        "\n",
        "[1] Artigo Medium: https://medium.com/@okanyenigun/langchain-in-chains-28-google-search-af86874bd29c\n",
        "\n",
        "[2] O que √© Faiss (Facebook AI Similarity Search)?: https://www.datacamp.com/pt/blog/faiss-facebook-ai-similarity-search"
      ],
      "metadata": {
        "id": "y5djmQwZvMnZ"
      }
    }
  ]
}